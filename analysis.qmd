---
title: "Applied Retail Forecasting"
format:
  html:
    toc: true
    toc-depth: 3
    number-sections: true
    code-fold: true
execute:
  echo: false
---

# Overview

This project applies classical time series forecasting techniques to Australian retail turnover data, with a focus on furniture and household goods retailing in the Northern Territory of Australia. Using monthly data published by the Australian Bureau of Statistics, the analysis aims to understand the underlying structure of the series and assess how different forecasting approaches perform in practice.

The workflow follows a standard applied forecasting process. The data are first explored to identify trends, seasonality, and structural changes, including the impact of major economic events such as COVID-19. Variance-stabilising transformations and decomposition techniques are then used to better understand the series before fitting and evaluating models.

Two model families are considered: Exponential Smoothing State Space (ETS) models and Seasonal ARIMA models. Model selection is guided by a combination of information criteria, residual diagnostics, and out-of-sample performance using a 24-month test set. Forecast accuracy is further assessed by comparing model predictions to subsequently released ABS data.

The goal of this project is not only to produce forecasts, but to demonstrate a transparent and defensible forecasting workflow, highlighting the trade-offs between model interpretability, stability, and responsiveness when applied to real-world economic data.

# Data

The analysis uses monthly retail turnover data from the Australian Bureau of Statistics (ABS) Retail Trade release (Table 11). The series corresponds to furniture, floor coverings, houseware and textile goods retailing in the Northern Territory, measured in AUD millions, spanning April 1988 to December 2022. A sample of observations is shown below.

```{r}
#| label: load-libraries
#| message: false

# Loading libraries

library(tidyverse)
library(fpp3)
library(janitor)
library(lubridate)
library(patchwork)
library(readabs)
library(yardstick)
library(gt)

```

```{r}
#| label: get-data
#| message: false
#| eval: false

# Get data from ABS
retail <- read_abs(cat_no = "8501.0", tables = 11) |>
  filter(series_id == "A3349526J") |>
  transmute(
    State = "Northern Territory",
    Industry = "Furniture, floor coverings, houseware and textile goods retailing",
    `Series ID` = series_id,
    Month = yearmonth(date),
    Turnover = value
  ) |>
  as_tsibble(index = Month)

# For simplicity purposes, the dataset will be saved to local
# write_csv(retail, "data/retail.csv")
# write_rds(retail, "data/retail.rds")

```


```{r}
#| label: load-and-clean-data

# For simplicity purposes, the dataset will be loaded from local
retail <- read_rds("data/retail.rds")

# Clean and prepare for analysis
retail_clean <- retail |> 
  clean_names() |> 
  drop_na() |>
  filter(month <= yearmonth("2022 Dec"))

```


```{r}
#| label: data-split

# Splitting the data into train and test sets

# Training set: up to and including 2020
retail_train <- retail_clean |> 
  slice_head(n = nrow(retail_clean) - 24)

# Test set: from 2020 onwards
retail_test <- retail_clean |> 
  slice_tail(n = 24)

```

```{r}
#| label: raw-data-snippet

gt(head(retail_clean))

```


# Exploratory Data Analysis (EDA)

Exploratory analysis is used to understand the overall structure of the retail turnover series before any modeling is performed. This includes examining long-term trends, seasonal patterns, and changes in variability over time, as well as identifying potential structural breaks linked to broader economic conditions.

A set of time series visualizations are used to assess these features. These plots help determine whether the series exhibits trend and seasonality, whether variance appears constant over time, and whether there are periods of unusually high or low volatility. Particular attention is given to changes around major events such as the mining boom period and the COVID-19 pandemic, both of which are expected to have influenced retail behaviour in the Northern Territory.

The insights from this exploratory stage inform later decisions around transformation, decomposition, and model selection.


## Trend, seasonality, and structural features

```{r}
#| label: fig-plot1
#| message: false
#| error: false
#| warning: false

# Plotting the dataset to see original structure
autoplot(retail_clean) + 
  theme_bw() +
  labs(title = "Autoplot of the Retail Dataset",
       x = "Month (1M)",
       y = "Turnover ($ Millions)")

```

```{r}
#| label: fig-plot2
#| message: false
#| error: false
#| warning: false

# Plotting a season plot to view seasonality by year
retail_clean |> 
  gg_season(turnover) +
  theme_bw() +
  labs(title = "Season Plot of the Retail Dataset",
       x = "Month (1M)",
       y = "Turnover ($ Millions)")

```

```{r}
#| label: fig-plot3
#| message: false
#| error: false
#| warning: false

# Plotting a subseries plot to see trend and patterns over each month 
# across all years
retail_clean |> gg_subseries(turnover) +
  theme_bw() +
  labs(title = "Subseries Plot of the Retail Dataset",
       x = "Month (1M)",
       y = "Turnover ($ Millions)") +
  theme(axis.text.x = element_text(angle = 60,
                                   hjust = 1))

```


The time series plot in @fig-plot1 shows a clear upward long-term trend, indicating sustained growth in retail turnover over the sample period. Seasonal behaviour is also evident, with fluctuations that increase in magnitude as the level of the series rises, suggesting multiplicative seasonality. A pronounced increase in turnover is observed between approximately 2007 and 2009, which likely reflects the effects of the Northern Territory mining boom during that period. Following this, turnover declines sharply post-2010, potentially linked to the global financial crisis and the subsequent slowdown in mining-related economic activity.

In the years leading up to 2020, turnover appears relatively subdued before rising again during and after the COVID-19 period. While many industries experienced contraction during this time, the increase observed here may reflect shifts in household behaviour, with greater emphasis on home-related spending as lifestyles adjusted. Given the broad scope of this retail category, multiple overlapping factors are likely influencing the observed dynamics.

The seasonal plot in @fig-plot2 highlights strong and consistent seasonal patterns across years. Turnover tends to peak in June and December, while lower values are commonly observed around February and April. The regular repetition of these patterns supports the presence of true seasonality rather than irregular cyclical effects. Although the exact drivers are not directly observed in the data, these patterns may be associated with factors such as end-of-financial-year spending, holiday periods, and seasonal changes in consumer behaviour.

The subseries plot in @fig-plot3 further reinforces these findings by displaying monthly behaviour across the full time span. Each month exhibits a distinct seasonal profile, while also reflecting the broader upward movement in turnover over time. June and December consistently record higher average turnover, whereas February and April remain comparatively weaker months. The horizontal reference lines representing monthly means make these differences particularly clear.

One notable feature in the subseries plot is an unusually high turnover value observed in July around 2007. This may represent a genuine economic anomaly linked to exceptional conditions during that period, although the possibility of a data irregularity cannot be ruled out. Overall, the exploratory analysis confirms the presence of trend, seasonality, and changing variance, all of which are important considerations for subsequent transformation and model selection.

## Data transformation and decomposition

This section focuses on preparing the series for modelling by addressing changing variance and separating the underlying components of the time series. These steps help ensure that subsequent models are applied to data that are more stable and interpretable.

### Variance stabilisation using the Box–Cox transformation

```{r}

# Extracting Guerrero lambda value for Box-Cox Transformation
lambda_retail <- retail_train |> 
  features(turnover, features = guerrero) |> 
  pull(lambda_guerrero)

```

```{r}

# Transforming the dataset with lambda 
bct_plot1 <- retail_train |> 
  autoplot(box_cox(turnover, lambda_retail)) +
  theme_bw() +
  labs(title = "Lambda Transformed Retail Dataset",
       x = "Month (1M)",
       y = "Lambda Transformed Turnover ($ Millions)") +
  theme(plot.title = element_text(size = 12))

```

```{r}

# Transforming the dataset with log 
bct_plot2 <- retail_train |> 
  autoplot(box_cox(turnover, 0)) +
  theme_bw() +
  labs(title = "Log Transformed Retail Datase",
       x = "Month (1M)",
       y = "Log Transformed Turnover ($ Millions)")  +
  theme(plot.title = element_text(size = 12))

```

```{r}
#| label: fig-plot4
#| message: false
#| error: false
#| warning: false

# Plotting both of the figures side by side
bct_plot1 + bct_plot2

```

To stabilize the variance of the series and address heteroskedasticity, a Box–Cox transformation is applied to the retail turnover data. The transformation parameter 
λ is selected using the Guerrero method, which chooses the value that minimizes variability across seasonal subseries. Applying this procedure yields an estimated λ value of 0.242.

@fig-plot4 compares the Box–Cox transformation using the Guerrero-estimated value with a log transformation, corresponding to λ = 0. Both transformations reduce variance instability relative to the original series; however, the log-transformed series exhibits slightly tighter peaks and troughs, indicating more consistent variance across seasonal periods. The estimated value being close to zero suggests that a log transformation is sufficient in practice.

Overall, comparison with the original series in @fig-plot1 indicates that the transformation is effective in achieving approximate variance stability, making the data more suitable for subsequent decomposition and time series modelling.

### Seasonal-trend decomposition using STL

```{r}
#| label: fig-plot5
#| message: false
#| error: false
#| warning: false

# Creating a new variable with the box_cox transformed data
retail_train <- retail_train |> 
  mutate(bc_turnover = box_cox(turnover, 0))

# Performing STL Decomposition
retail_train |> 
  model(stl = STL(bc_turnover ~ season(window = 13) +
                    trend(window = 25),
                    robust = TRUE)) |> 
  components() |> 
  autoplot() +
  theme_bw() +
  labs(title = "STL Decomposition | season=13 & trend=25",
       x = "Month (1M)")

```

Following variance stabilization, the transformed series is decomposed using STL to separate the trend, seasonal, and remainder components. This helps assess whether seasonality is stable over time and whether any meaningful structure remains unexplained.

Because the series spans multiple decades, STL parameters are adjusted rather than relying on defaults. The seasonal window is set to 13 to allow the seasonal component to vary smoothly while still reflecting an annual cycle, which is appropriate for monthly retail data where recurring patterns such as end-of-financial-year and holiday periods are expected. The trend window is set to 25 to capture medium-term movements (roughly two years), allowing the trend component to reflect gradual shifts associated with changing economic conditions in the Northern Territory.

Given the presence of irregular spikes and sharp movements, particularly between 2007 and 2022, `robust = TRUE` is used to reduce the influence of extreme observations on the estimated components. With these settings, the decomposition in @fig-plot5 shows a well-defined long-term trend, a stable seasonal pattern, and a remainder that appears largely noise-like, suggesting that the major structure of the series has been captured.

Overall, the STL decomposition confirms that this turnover series is characterized by a persistent long-run trend and consistent seasonal variation year to year, supporting the use of seasonal forecasting models in later sections.

### Stationarity and differencing

```{r}
#| label: tbl-table1

## KPSS Test Before transformation

retail_train |> 
  select(-c(state, industry, turnover)) |> 
  features(bc_turnover, unitroot_kpss)

```

```{r}
#| label: tbl-table2

## Number of seasonal differences recommended

retail_train |> 
  select(-c(state, industry, turnover)) |> 
  features(bc_turnover, unitroot_nsdiffs)

```


```{r}
#| label: tbl-table3

## Number of regular differences recommended

retail_train |> 
  select(-c(state, industry, turnover)) |> 
  features(bc_turnover, unitroot_ndiffs)

```


```{r}
#| label: tbl-table4

## KPSS Test after transformation 

retail_train <- retail_train |>
  mutate(transformed = box_cox(turnover, 0) |>
    difference(12) |>
    difference(1))

retail_train |>
  select(-c(state, industry)) |>
  features(transformed, unitroot_kpss)

```

```{r}
#| label: fig-plot6
#| message: false
#| error: false
#| warning: false

# Plotting the data after seasonal differencing on 
# the box cox transformed dataset

retail_train |> 
  autoplot(bc_turnover |> 
             difference(12)) +
  theme_bw() +
  labs(title = "Seasonally Differenced Retail Dataset",
       x = "Month (1M)",
       y = "Turnover ($ Millions)")

```


```{r}
#| label: fig-plot7
#| message: false
#| error: false
#| warning: false

# Plotting the data after first order differencing on
# the box cox transformed dataset

retail_train |> 
  autoplot(bc_turnover |> 
             difference(12) |> 
             difference(1)) +
  theme_bw() +
  labs(title = "Seasonally and First Differenced Retail Dataset",
       x = "Month (1M)",
       y = "Turnover ($ Millions)")


```

Before fitting ARIMA models, the series must be made stationary by removing any remaining trend and seasonal structure. Visual inspection of the original series in @fig-plot1 confirms the presence of both trend and seasonality, while the Box–Cox transformation applied earlier successfully stabilizes variance, as seen in @fig-plot4.

To formally assess stationarity and guide differencing decisions, a set of unit root tests are applied to the transformed series. The KPSS test results in @tbl-table1 return a p-value of 0.01, providing evidence against stationarity in the undifferenced series. Automated differencing diagnostics suggest applying one seasonal difference (@tbl-table2) and one regular difference (@tbl-table3).

After applying both a seasonal difference of order 12 and a first-order difference, the KPSS test is repeated on the fully differenced series. The resulting p-value of 0.1 in @tbl-table4 indicates that there is no strong evidence against stationarity, suggesting that the differencing strategy has been effective.

This progression is also reflected visually. The seasonally differenced series shown in @fig-plot6 no longer exhibits clear seasonal patterns, but still displays some residual structure. Applying an additional first-order difference yields the series in @fig-plot7, which fluctuates around a constant mean with no obvious trend or seasonality, consistent with a stationary process.

Overall, these results indicate that one seasonal difference and one regular difference are sufficient to achieve stationarity, providing an appropriate foundation for ARIMA model identification in the next section.

# Model Identification and Selection

## Candidate model specification

### Exponential smoothing (ETS) models

Based on the exploratory analysis, the series exhibits a clear trend, strong seasonality, increasing variance, and occasional sharp movements. These characteristics motivate the use of exponential smoothing models that can accommodate trend damping and multiplicative effects.

A set of candidate ETS models is considered. All models use a damped additive trend to reflect the expectation that long-run growth may flatten over time, particularly given the observed fluctuations in the series. The shortlisted models include:

1) an additive Holt–Winters specification, 
2) a hybrid model with multiplicative errors and additive seasonality, 
3) a fully multiplicative specification, 
4) and an automatically selected ETS model.

These candidates allow comparison between different assumptions about how variance and seasonal effects scale with the level of the series.

### ARIMA model identification

```{r}
#| label: fig-plot8
#| message: false
#| error: false
#| warning: false

# Plotting the ACF and PACF Plots for ARIMA Model Identification
retail_train |>
  gg_tsdisplay(transformed, plot_type = "partial", lag = 36) +
  labs(title = "ACF and PACF Plots For ARIMA Model Identification")

```

ARIMA model identification is guided by the autocorrelation and partial autocorrelation functions of the transformed and differenced series, shown in @fig-plot8. One regular difference and one seasonal difference are retained, consistent with the stationarity analysis in the previous section, resulting in a baseline structure of (0,1,0)(0,1,0)[12].

Inspection of the ACF suggests the presence of a non-seasonal moving average component at lag 1, while the PACF indicates a potential non-seasonal autoregressive component at lag 2. At the seasonal level, a significant spike at lag 12 in the ACF suggests a seasonal MA(1) term, while spikes at lags 12 and 24 in the PACF indicate a possible seasonal AR(2) structure.

Based on these patterns, two candidate ARIMA specifications are selected: (2,1,0)(2,1,0)[12] and (2,1,0)(0,1,1)[12], alongside an automatically selected ARIMA model.


# Model comparison using information criteria

## ETS model comparison

```{r}

# Fitting ETS models on raw (untransformed) data
ets_models <- retail_train |>
  model(
    ETS_AAdA = ETS(turnover ~ error("A") + trend("Ad") + season("A")),
    ETS_MAdA = ETS(turnover ~ error("M") + trend("Ad") + season("A")),
    ETS_MAdM = ETS(turnover ~ error("M") + trend("Ad") + season("M")),
    ETS_auto = ETS(turnover) 
  )

# Checking which model the Auto ETS selected
ets_models |> 
  select(ETS_auto) |> 
  report()

```


```{r}
#| label: tbl-table5

# Compare AIC values and model details
glance(ets_models) |> 
  # select(-c(state, industry)) |> 
  arrange(AIC)

```

The candidate ETS models are fitted to the training data and compared using the Akaike Information Criterion (AIC), with results summarized in @tbl-table5. The automatically selected ETS model, ETS(M,A,M), achieves the lowest AIC, indicating the best in-sample fit among the specifications considered.

However, the damped-trend models ETS(M,Ad,A) and ETS(M,Ad,M) perform comparably in terms of AIC, with only marginally higher values. Given their damped trend structure, these models remain plausible alternatives, particularly if long-run growth is expected to slow.

**Note: ETS models are fitted to the original turnover series, as exponential smoothing methods natively accommodate trend and seasonality without requiring prior differencing or transformation.**

## ARIMA model comparison

```{r}
#| label: tbl-table6
#| cache: true

# Modelling the selected models and the auto selected ARIMA model
# Using stepwise and approximation = false to allow the function
# to search deeper and harder for the best fitting model

arima_models <- retail_train |>
  model(
    model1 = ARIMA(box_cox(turnover, 0) ~ pdq(2, 1, 0) + PDQ(2, 1, 0)),
    model2 = ARIMA(box_cox(turnover, 0) ~ pdq(2, 1, 0) + PDQ(0, 1, 1)),
    auto = ARIMA(box_cox(turnover, 0), stepwise = FALSE, approximation = FALSE)
  )

# Clean printing the models and auto selected model.

arima_models |> 
  # select(-state, -industry) |> 
  pivot_longer(everything(),
  names_to = "Model name",
  values_to = "Orders"
)

```

```{r}
#| label: tbl-table7

# Viewing model performance statistics, arranged by AIC

glance(arima_models) |> 
  arrange(AIC) |> 
  select(.model:BIC)

```


A similar comparison is conducted for the ARIMA candidates. The automatically selected ARIMA model matches the manually specified (2,1,0)(0,1,1)[12] model, as shown in @tbl-table6. Both specifications achieve the lowest AIC values in @tbl-table7, indicating strong in-sample performance relative to the alternative candidate.

## Test-set evaluation and final model selection

```{r}

# Generate forecasts for the next 24 months
forecasts_ets <- ets_models |> 
  forecast(new_data = retail_test)

forecasts_arima <- arima_models |> 
  forecast(new_data = retail_test)

# Evaluate forecast accuracy for ETS models
accuracy_ets <- fabletools::accuracy(forecasts_ets, retail_test) |> 
  # select(-c(state, industry)) |> 
  arrange(RMSE)

# Evaluate forecast accuracy for ARIMA models
accuracy_arima <- fabletools::accuracy(forecasts_arima, retail_test) |> 
  # select(-c(state, industry)) |> 
  arrange(RMSE)

```

```{r}
#| label: tbl-table8

accuracy_ets
```

```{r}
#| label: tbl-table9

accuracy_arima

```

To assess out-of-sample performance, all shortlisted ETS and ARIMA models are evaluated on a 24-month test set. Forecast accuracy metrics for ETS and ARIMA models are reported in @tbl-table8 and @tbl-table9, respectively.

For the ARIMA models, the (2,1,0)(0,1,1)[12] specification achieves both the lowest AIC and the lowest test-set RMSE, and is also selected automatically by the ARIMA procedure. This consistency provides strong support for its selection as the preferred ARIMA model.

For the ETS models, the automatically selected ETS(M,A,M) model achieves the lowest AIC but performs less well on the test set. In contrast, ETS(M,Ad,A) delivers the lowest test-set RMSE among ETS candidates while maintaining a competitive AIC value. Given its superior out-of-sample performance and damped trend structure, ETS(M,Ad,A) is selected as the preferred ETS model.

These two models are carried forward for detailed diagnostics and forecasting in the next section.

# Model diagnostics and forecasting

## Parameter estimates

```{r}
#| label: tbl-table10

# ETS model summary
ets_models |> 
  select(ETS_MAdA) |> 
  report()

```

```{r}
#| label: tbl-table11

# ARIMA model summary
arima_models |> 
  select(model2) |> 
  report()

```


@tbl-table10 reports the parameter estimates for the selected ETS(M,Ad,A) model. The level smoothing parameter (α = 0.43) indicates that the model places moderate weight on recent observations when updating the series level. The trend smoothing parameter is close to zero, consistent with the use of a damped trend that evolves slowly over time. The seasonal smoothing parameter (γ = 0.096) suggests relatively stable seasonal patterns, with only gradual updating as new data arrive. The damping parameter (ϕ = 0.98) confirms that the trend is expected to persist in the short term but gradually flatten in the long run.

Parameter estimates for the selected ARIMA(2,1,0)(0,1,1)[12] model are shown in @tbl-table11. The negative autoregressive coefficients indicate mild mean-reverting behaviour, where deviations tend to be followed by partial corrections. The seasonal moving average term captures strong dependence at the annual lag, reflecting the importance of shocks occurring at the same time in previous years. Together, these parameters indicate a model that responds to both short-term fluctuations and seasonal structure.


## Residual diagnostics


```{r}
#| label: fig-plot9
#| warning: false

# ETS residual diagnostics
ets_models |> 
  select(ETS_MAdA) |> 
  gg_tsresiduals() +
  labs(title = "Residual Plots for ETS Model")

```

```{r}
#| label: tbl-table12

# ETS Ljung-Box test
ets_models |> 
  select(ETS_MAdA) |> 
  augment() |> 
  features(.resid, ljung_box, lag = 24, dof = 3)

```

```{r}
#| label: fig-plot10

# ARIMA residual diagnostics
arima_models |> 
  select(model2) |> 
  gg_tsresiduals()  +
  labs(title = "Residual Plots for ARIMA Model")

```

```{r}
#| label: tbl-table13

# ARIMA Ljung-Box test
arima_models |> 
  select(model2) |> 
  augment() |> 
  features(.resid, ljung_box, lag = 24, dof = 3)

```

Residual diagnostics are used to assess whether the fitted models have adequately captured the systematic structure in the data. For the ETS model, the residual plots in @fig-plot9 show several autocorrelation spikes beyond the 95% confidence bounds. This is supported by the Ljung–Box test results in @tbl-table12, which return a very small p-value at lag 24, providing strong evidence against the residuals being white noise. While this suggests that some autocorrelation remains unmodelled, this behaviour is not uncommon for ETS models and does not necessarily preclude good forecasting performance.

In contrast, the residual diagnostics for the ARIMA model in @fig-plot10 show no substantial autocorrelation remaining, with only a single spike marginally exceeding the confidence bounds. The Ljung–Box test in @tbl-table13 returns a p-value of 0.069, indicating no strong evidence of residual autocorrelation. This suggests that the ARIMA model has captured the underlying dependence structure effectively and produces residuals consistent with white noise.

## Forecasts and prediction intervals

```{r}
#| label: fig-plot11

# Combining the models into one "fit" object 
fit <- retail_train |>
  model(
    final_ets = ETS(turnover ~ error("M") + trend("Ad") + season("A")),
    final_arima = ARIMA(box_cox(turnover, 0) ~ pdq(2, 1, 0) + PDQ(0, 1, 1))
  )

# Forecasting
fc_plot <- fit |> 
  forecast(h = "2 years")


# Plot forecasts
fc_plot |> 
  autoplot(retail_clean, size = 1) + 
  theme_bw() +
  labs(title = "Forecasts from Final ETS and ARIMA Models", 
       y = "Turnover ($ Millions)", 
       x = "Year")


```

Forecasts from the selected ETS and ARIMA models are shown in @fig-plot11. Both models generate similar short-term forecasts and capture the recurring seasonal pattern in the data. However, differences emerge in their treatment of longer-term uncertainty.

The ETS model produces smoother forecasts with relatively narrow prediction intervals, reflecting its damped trend structure and assumption of stable long-run behaviour. In contrast, the ARIMA model produces wider prediction intervals, indicating greater sensitivity to recent variability and uncertainty in future dynamics. While both forecasts are plausible, the ARIMA model reflects a higher degree of uncertainty, whereas the ETS model provides more conservative and stable projections.

## Test-set comparison

Both the ETS and ARIMA models perform well on the 24-month test set, capturing the underlying seasonal structure and overall movement in retail turnover. However, differences emerge when comparing forecast accuracy and uncertainty.

The ETS(M,Ad,A) model achieves the lowest RMSE on the test set, as shown in @tbl-table8 and @tbl-table9, indicating superior out-of-sample accuracy. Its forecasts are smoother and more stable, reflecting the damped trend structure and assumption of gradual long-run behaviour.

In contrast, the ARIMA(2,1,0)(0,1,1)[12] model achieves a slightly lower in-sample AIC and responds more strongly to recent fluctuations in the data. While this flexibility allows it to adapt quickly to changes, it also results in higher test-set RMSE and wider prediction intervals, indicating greater forecast uncertainty.

Overall, the ETS model provides more stable and reliable forecasts in an out-of-sample setting, while the ARIMA model may be preferable in environments where rapid changes and short-term volatility are of primary interest.


## Final forecasts using full data

```{r}
#| label: fig-plot12

# Refitting the models to the full dataset
fit_full <- retail_clean |> 
  model(
    final_ets = ETS(turnover ~ error("M") + trend("Ad") + season("A")),
    final_arima = ARIMA(box_cox(turnover, 0) ~ pdq(2, 1, 0) + PDQ(0, 1, 1))
  )

# Forecasting for 2 years after end of dataset
forecast_full <- fit_full |> 
  forecast(h = "2 years")

# Plotting forecasts
autoplot(retail_clean, turnover) +
  autolayer(forecast_full, size = 1, level = 80) +
  theme_bw() +
  labs(title = "Two-year forecasts from refitted ETS and ARIMA models",
       y = "Turnover ($ Millions)",
       x = "Year")

```

To produce final forecasts, both selected models are refitted to the full dataset using the same model structures identified earlier. Parameter estimates are re-estimated, but no changes are made to the model specifications.

Figure @fig-plot12 shows two-year ahead forecasts from the refitted ETS and ARIMA models, along with 80% prediction intervals. Both models project a continuation of recent trends and seasonal patterns, while differing in the degree of forecast uncertainty. The ETS forecasts remain relatively smooth and constrained, whereas the ARIMA forecasts allow for a wider range of possible outcomes, reflecting their sensitivity to recent variability.

These final forecasts illustrate how each modelling approach extrapolates beyond the observed data and highlight the trade-off between forecast stability and responsiveness.

# Forecast validation using updated ABS data

```{r}
#| message: false
#| error: false
#| warning: false

# Extracting ABS data
abs_data <- read_abs(cat_no = "8501.0", tables = 11) |> 
  filter(series_id == "A3349526J")

```


```{r}

# Prepare the data for testing
abs_actuals_ts  <- abs_data |> 
  mutate(month = yearmonth(date)) |> 
  select(month, actual_value = value) |> 
  filter(month >= yearmonth("1988 April")) |> 
  as_tsibble(index = month)


# Filtering for forecasted period which was after full dataset period
abs_actuals <- abs_actuals_ts |> 
  filter(month >= yearmonth("2023 Jan")) |> 
  filter(month <= yearmonth("2024 Dec"))

# Extracting forecast values from Task 9
forecast_vals <- forecast_full |> 
  as_tibble() |> 
  filter(month >= yearmonth("2023 Jan"), 
         month <= yearmonth("2024 Dec")) |> 
  select(.model, month, .mean)

```


```{r}
#| label: tbl-table14

# Join with ABS actuals
comparison <- left_join(forecast_vals, abs_actuals, by = "month")

# Computing accuracy measures 
comparison_results <- comparison |> 
  group_by(.model) |> 
  summarise(
    RMSE = rmse_vec(truth = actual_value, estimate = .mean),
    MAE  = mae_vec(truth = actual_value, estimate = .mean),
    MAPE = mape_vec(truth = actual_value, estimate = .mean)
  )

comparison_results

```

```{r}
#| label: fig-plot13
#| message: false
#| error: false
#| warning: false

# Combine forecasts and actuals
plot_data <- forecast_full |> 
  as_tibble() |> 
  filter(month >= yearmonth("2023 Jan"), 
         month <= yearmonth("2024 Dec")) |> 
  select(month, .model, .mean) |> 
  left_join(abs_actuals, by = "month")

# Plotting for comparison
ggplot(plot_data, aes(x = month)) +
  geom_line(aes(y = .mean, color = .model), 
            size = 1) +
  geom_line(aes(y = actual_value), 
            color = "black", linetype = "dashed", size = 1) +
  labs(title = "Forecasts vs ABS Actuals (2023–2024)",
       y = "Turnover ($ Millions)",
       x = "Month",
       color = "Model") +
  theme_bw()

```

To evaluate how the selected models perform in a real-world setting, updated retail turnover data are obtained from the Australian Bureau of Statistics (ABS) for the period following the end of the original sample. This allows the forecasts produced in the previous section to be compared against realised values that were not available at the time the models were estimated.

@fig-plot13 and the accuracy metrics reported in @tbl-table14 show that both models capture the underlying seasonal structure reasonably well over the 2023–2024 period. However, the ARIMA model consistently outperforms the ETS model across all reported measures. In particular, the ARIMA model achieves lower RMSE, MAE, and MAPE values (RMSE = 0.641 compared to 0.804 for ETS), indicating superior short-term accuracy during this period.

Visually, the ARIMA forecasts track the observed ABS values more closely, especially during the first half of 2024, where the ETS model tends to underpredict turnover. This suggests that the ARIMA model is better able to adapt to recent volatility and short-term shifts in the series, whereas the ETS model produces smoother and more conservative projections.

These results highlight an important trade-off observed throughout the analysis. While ETS models provide stable and reliable forecasts under relatively smooth conditions, ARIMA models may offer improved performance when the underlying dynamics of the series change more rapidly. In practice, model choice should therefore be guided by the forecasting horizon and the expected stability of the economic environment.

# Model strengths, limitations, and operational use

## Strengths and limitations of the selected models

Both the ETS and ARIMA models demonstrate clear strengths when applied to this retail turnover series, but each also has limitations. The ETS model provides a structured and interpretable framework for modelling level, trend, and seasonality. Its forecasts are stable, smooth, and easy to communicate, making it a strong choice when consistency and interpretability are prioritised. However, this same structure can cause the model to respond slowly to sudden changes, particularly during periods of increased volatility or structural shifts.

The ARIMA model, in contrast, is more reactive to recent movements in the data. This responsiveness allowed it to outperform ETS in terms of forecast accuracy when evaluated against realised ABS data for 2023–2024. That said, its forecasts are less smooth and its behaviour can be harder to interpret, especially when multiple autoregressive and moving average components interact. While ARIMA achieved better numerical accuracy in this case, neither model fully captured all short-term fluctuations in the series, reflecting the inherent noise and complexity of retail turnover data.

Overall, the results highlight a key trade-off common in applied forecasting. ETS models offer clarity and stability, while ARIMA models provide flexibility and responsiveness. The “better” model depends on whether the forecasting objective prioritises interpretability or short-term accuracy.

## Forecasting workflow in practice

If forecasts of this series were required on a recurring annual basis, a structured and repeatable workflow would be essential. Each year would begin by updating the dataset with the latest ABS release, ensuring consistency in definitions, frequency, and transformations. The updated series would then be reviewed visually and statistically to assess changes in trend, seasonality, or variance, supported by tools such as time series plots and STL decomposition.

Rather than reusing a fixed model indefinitely, the data structure would be reassessed annually. ETS and ARIMA models would be re-evaluated using information criteria, residual diagnostics, and recent out-of-sample performance to ensure the selected specification remains appropriate. Once a suitable model is identified, it would be refitted to the full, updated dataset and used to generate forecasts for the following 12 to 24 months, accompanied by 80% and 95% prediction intervals to communicate uncertainty.

Finally, forecast accuracy would be monitored by comparing previous forecasts to realised outcomes using metrics such as RMSE or MAPE. Persistent deterioration in performance would trigger a more thorough reassessment of the modelling approach, while still maintaining a consistent and interpretable forecasting framework. This process balances methodological discipline with flexibility, allowing forecasts to remain relevant as economic conditions evolve.

# References

Hyndman, R. J., & Athanasopoulos, G. (2021). Forecasting: Principles and practice (3rd ed.). OTexts. [https://otexts.com/fpp3/](https://otexts.com/fpp3/)

Hadley Wickham, Mara Averick, Jennifer Bryan, Winston Chang, Lucy D’Agostino McGowan, Romain François, Garrett Grolemund, et al. (2019).
Welcome to the tidyverse. Journal of Open Source Software, 4(43), 1686. [https://cran.r-project.org/web/packages/tidyverse](https://cran.r-project.org/web/packages/tidyverse)

Rob J Hyndman, Earo Wang (2020).
fpp3: Data for “Forecasting: Principles and Practice” (3rd edition) [R package]. [https://CRAN.R-project.org/package=fpp3](https://CRAN.R-project.org/package=fpp3)

Sam Firke (2023).
janitor: Simple tools for examining and cleaning dirty data [R package]. [https://CRAN.R-project.org/package=janitor](https://CRAN.R-project.org/package=janitor)

Garrett Grolemund, Hadley Wickham (2011).
Dates and times made easy with lubridate. Journal of Statistical Software, 40(3), 1–25. [https://cran.r-project.org/web/packages/lubridate](https://cran.r-project.org/web/packages/lubridate)

Thomas Lin Pedersen (2023).
patchwork: The Composer of Plots [R package]. [https://CRAN.R-project.org/package=patchwork](https://CRAN.R-project.org/package=patchwork)

Matt Cowgill (2024).
readabs: Download and Tidy Time Series Data from the Australian Bureau of Statistics (ABS) [R package]. [https://CRAN.R-project.org/package=readabs](https://CRAN.R-project.org/package=readabs)

Max Kuhn, Davis Vaughan, Romain François, Daniel Falbel, and Jeroen Ooms (2023).
yardstick: Tidy Characterizations of Model Performance [R package]. [https://CRAN.R-project.org/package=yardstick](https://CRAN.R-project.org/package=yardstick)

